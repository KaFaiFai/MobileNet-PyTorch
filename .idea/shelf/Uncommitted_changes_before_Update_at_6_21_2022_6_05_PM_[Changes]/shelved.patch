Index: script/test_loop.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import numpy as np\r\nimport torch\r\nfrom torch.nn import Module\r\nfrom torch.utils.data import DataLoader\r\n\r\nfrom tools import ClassificationMetrics\r\n\r\n\r\ndef test_loop(network: Module, dataloader: DataLoader, criterion: Module, **kwargs):\r\n    device = kwargs[\"device\"]\r\n    test_print_step = kwargs[\"test_print_step\"]\r\n    num_batches = len(dataloader)\r\n    digits = int(np.log10(num_batches)) + 1  # for print\r\n\r\n    total_loss = 0  # BCE loss\r\n    all_labels = []\r\n    all_outputs = []\r\n    with torch.no_grad():\r\n        for batch_idx, (images, labels) in enumerate(dataloader):\r\n            images, labels = images.to(device), labels.to(device)\r\n\r\n            network.eval()\r\n            outputs = network(images)\r\n            loss = criterion(outputs, labels)\r\n\r\n            total_loss += loss.item()\r\n\r\n            all_labels += labels.tolist()\r\n            all_outputs += outputs.tolist()\r\n\r\n            metrics = ClassificationMetrics(labels, outputs)\r\n            if test_print_step is not None and batch_idx % test_print_step == 0:\r\n                print(\r\n                    f\"[Batch {batch_idx:{digits}d}/{num_batches}] \"\r\n                    f\"Loss: {loss.item():.4f}, \"\r\n                    f\"Accuracy: {metrics.accuracy:.2%}\")\r\n\r\n    total_loss /= len(dataloader)\r\n    print(f\"Total test data: {len(all_labels)}, Loss: {total_loss:.4f}\")\r\n    metrics = ClassificationMetrics(all_labels, all_outputs)\r\n    metrics.print_report()\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/script/test_loop.py b/script/test_loop.py
--- a/script/test_loop.py	(revision 11f2caf4762201f1e4e6ca9dd1ab2f33af88b09f)
+++ b/script/test_loop.py	(date 1655805659866)
@@ -8,7 +8,7 @@
 
 def test_loop(network: Module, dataloader: DataLoader, criterion: Module, **kwargs):
     device = kwargs["device"]
-    test_print_step = kwargs["test_print_step"]
+    print_step_test = kwargs["print_step_test"]
     num_batches = len(dataloader)
     digits = int(np.log10(num_batches)) + 1  # for print
 
@@ -29,7 +29,7 @@
             all_outputs += outputs.tolist()
 
             metrics = ClassificationMetrics(labels, outputs)
-            if test_print_step is not None and batch_idx % test_print_step == 0:
+            if print_step_test is not None and batch_idx % print_step_test == 0:
                 print(
                     f"[Batch {batch_idx:{digits}d}/{num_batches}] "
                     f"Loss: {loss.item():.4f}, "
Index: script/train_epoch.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import torch\r\nfrom torch.nn import Module\r\nfrom torch.utils.data import DataLoader\r\nfrom torch.optim import Optimizer\r\n\r\nimport numpy as np\r\nimport timeit\r\n\r\nfrom tools import ClassificationMetrics\r\n\r\n\r\ndef train_epoch(network: Module, dataloader: DataLoader, optimizer: Optimizer, criterion: Module, **kwargs):\r\n    start = timeit.default_timer()\r\n    device, train_print_step = kwargs[\"device\"], kwargs[\"train_print_step\"]\r\n    num_batches = len(dataloader)\r\n    digits = int(np.log10(num_batches)) + 1  # for print\r\n\r\n    for batch_idx, (images, labels) in enumerate(dataloader):\r\n        images, labels = images.to(device), labels.to(device)\r\n\r\n        network.train()\r\n        outputs = network(images)\r\n        loss = criterion(outputs, labels)\r\n        optimizer.zero_grad()\r\n        loss.backward()\r\n        optimizer.step()\r\n\r\n        network.eval()\r\n        metrics = ClassificationMetrics(labels, outputs)\r\n        if batch_idx % train_print_step == 0:\r\n            print(\r\n                f\"[Batch {batch_idx:{digits}d}/{num_batches}] \"\r\n                f\"Loss: {loss.item():.4f}, \"\r\n                f\"Accuracy: {metrics.accuracy:.2%}\")\r\n\r\n    end = timeit.default_timer()\r\n    print(f\"Time spent: {end - start:.2f}s | {(end - start) / num_batches:.2f}s/batch\")\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/script/train_epoch.py b/script/train_epoch.py
--- a/script/train_epoch.py	(revision 11f2caf4762201f1e4e6ca9dd1ab2f33af88b09f)
+++ b/script/train_epoch.py	(date 1655805631862)
@@ -11,7 +11,7 @@
 
 def train_epoch(network: Module, dataloader: DataLoader, optimizer: Optimizer, criterion: Module, **kwargs):
     start = timeit.default_timer()
-    device, train_print_step = kwargs["device"], kwargs["train_print_step"]
+    device, print_step_train = kwargs["device"], kwargs["print_step_train"]
     num_batches = len(dataloader)
     digits = int(np.log10(num_batches)) + 1  # for print
 
@@ -27,7 +27,7 @@
 
         network.eval()
         metrics = ClassificationMetrics(labels, outputs)
-        if batch_idx % train_print_step == 0:
+        if print_step_train is not None and batch_idx % print_step_train == 0:
             print(
                 f"[Batch {batch_idx:{digits}d}/{num_batches}] "
                 f"Loss: {loss.item():.4f}, "
Index: train.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from pathlib import Path\r\n\r\nimport torch\r\nfrom torch import optim\r\nfrom torch import nn\r\nfrom torch.utils.data import DataLoader\r\n\r\nfrom model import *\r\nfrom dataset import *\r\nfrom script import train_epoch, test_loop\r\nfrom script.utils import find_next_id, be_deterministic\r\n\r\nbe_deterministic()\r\n\r\n\r\ndef train():\r\n    # hyper parameters\r\n    batch_size = 64\r\n    lr = 3e-4\r\n    alpha = 1\r\n    input_resolution = 224\r\n\r\n    # experiment settings\r\n    data = \"cifar10\"  # [\"dogs\", \"mnist\", \"cifar10\", \"imagenet\"]\r\n    model_type = \"mobile_net\"  # [\"mobile_net\", \"lenet\"]\r\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\n    num_workers = 4\r\n    num_epochs = 50\r\n    save_step = 1\r\n    out_directory = r\".\\out\"\r\n    pretrained_model_path = None\r\n    # pretrained_model_path = r\"C:\\_Project\\Pycharm Projects\\MobileNet\\pretrained\\wjc-mobilenet-a100-r224-c1000-e0000.pth\"\r\n\r\n    assert data in [\"dogs\", \"mnist\", \"cifar10\", \"imagenet\"]\r\n    assert model_type in [\"mobile_net\", \"lenet\"]\r\n\r\n    # training configs\r\n    c = dict()\r\n    c[\"train_print_step\"] = 100\r\n    c[\"device\"] = device\r\n\r\n    out_path = Path(out_directory) / f\"{find_next_id(Path(out_directory)):04d}\"\r\n\r\n    # select dataset\r\n    train_dataset = None\r\n    test_dataset = None\r\n    if data == \"dogs\":\r\n        train_dataset = DogsDataset(root=r\"D:\\_Dataset\\Stanford Dogs\", is_train=True)\r\n        test_dataset = DogsDataset(root=r\"D:\\_Dataset\\Stanford Dogs\", is_train=False)\r\n    elif data == \"mnist\":\r\n        train_dataset = MNISTDataset(root=r\"D:\\_Dataset\", is_train=True)\r\n        test_dataset = MNISTDataset(root=r\"D:\\_Dataset\", is_train=False)\r\n    elif data == \"cifar10\":\r\n        train_dataset = CIFAR10Dataset(root=r\"D:\\_Dataset\\CIFAR10\", is_train=True)\r\n        test_dataset = CIFAR10Dataset(root=r\"D:\\_Dataset\\CIFAR10\", is_train=False)\r\n    elif data == \"imagenet\":\r\n        train_dataset = ImageNetDataset(root=r\"D:\\_Dataset\\ImageNet_2012\", is_train=True)\r\n        test_dataset = ImageNetDataset(root=r\"D:\\_Dataset\\ImageNet_2012\", is_train=False)\r\n    num_class = train_dataset.num_labels\r\n\r\n    # set up model, dataloader, optimizer, criterion\r\n    network = None\r\n    if model_type == \"lenet\":\r\n        network = LeNet(num_class).to(device)\r\n    elif model_type == \"mobile_net\":\r\n        network = MobileNet(num_class).to(device)\r\n    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\r\n    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers)\r\n    optimizer = optim.Adam(network.parameters(), lr=lr)\r\n    criterion = nn.CrossEntropyLoss()\r\n\r\n    from_epoch = 0\r\n    if pretrained_model_path is not None:\r\n        state = torch.load(pretrained_model_path)\r\n        network.load_state_dict(state[\"state_dict\"])\r\n        from_epoch = max(state[\"epoch\"] + 1, 0)\r\n\r\n    # training loop\r\n    for epoch in range(from_epoch, num_epochs):\r\n        print(f\"{'-' * 10} Epoch {epoch:2d}/{num_epochs} {'-' * 10}\")\r\n        train_epoch(network, train_dataloader, optimizer, criterion, **c)\r\n\r\n        print(f\"{'-' * 5} Validation result {'-' * 5}\")\r\n        test_loop(network, test_dataloader, criterion, **c)\r\n\r\n        if epoch % save_step == 0:\r\n            out_path.mkdir(exist_ok=True, parents=True)\r\n            save_to = out_path / f\"{model_type}-a{alpha * 100:3d}-r{input_resolution:d}-c{num_class}-e{epoch:04d}.pth\"\r\n            print(f\"{'-' * 5} Saving model to {save_to} {'-' * 5}\")\r\n            state = {\"epoch\": epoch, \"alpha\": alpha, \"input_resolution\": input_resolution,\r\n                     \"num_class\": num_class, \"state_dict\": network.state_dict()}\r\n            torch.save(state, str(save_to))\r\n\r\n        print('\\n')\r\n\r\n\r\nif __name__ == '__main__':\r\n    train()\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/train.py b/train.py
--- a/train.py	(revision 11f2caf4762201f1e4e6ca9dd1ab2f33af88b09f)
+++ b/train.py	(date 1655805618131)
@@ -1,69 +1,76 @@
 from pathlib import Path
+import argparse
 
 import torch
 from torch import optim
 from torch import nn
 from torch.utils.data import DataLoader
 
-from model import *
-from dataset import *
+from config import *
 from script import train_epoch, test_loop
 from script.utils import find_next_id, be_deterministic
 
 be_deterministic()
 
+parser = argparse.ArgumentParser()
+# required settings
+parser.add_argument("--data", type=str, help="dataset", choices=DATASETS.keys(), required=True)
+parser.add_argument("--model", type=str, help="model type", choices=MODELS.keys(), required=True)
+parser.add_argument("--batch-size", type=int, help="training batch size", required=True)
+
+# hyper parameters
+parser.add_argument("--lr", type=float, help="training learning rate", default=3e-4)
+parser.add_argument("--alpha", type=float, help="width multiplier of MobileNet", default=1.0)
+parser.add_argument("--input-resolution", type=int, help="input resolution of MobileNet", default=224)
+
+# misc parameters
+parser.add_argument("--device", type=str, help="cpu or gpu?", choices=["cpu", "cuda"], default="cuda")
+parser.add_argument("--num-workers", type=int, help="sub-processes for data loading", default=0)
+parser.add_argument("--num-epochs", type=int, help="training epochs", default=50)
+
+# misc settings
+parser.add_argument("--print-step", type=int, help="How often to print progress (in batch)?")
+parser.add_argument("--save-step", type=int, help="How often to save network (in epoch)?")
+parser.add_argument("--out-dir", type=Path, help="Where to save network (in epoch)?")
+parser.add_argument("--resume", type=Path, help="path to saved network")
 
-def train():
+
+def train(configs):
     # hyper parameters
-    batch_size = 64
-    lr = 3e-4
-    alpha = 1
-    input_resolution = 224
+    batch_size = configs.batch_size
+    lr = configs.lr
+    alpha = configs.alpha
+    input_resolution = configs.input_resolution
 
     # experiment settings
-    data = "cifar10"  # ["dogs", "mnist", "cifar10", "imagenet"]
-    model_type = "mobile_net"  # ["mobile_net", "lenet"]
-    device = "cuda" if torch.cuda.is_available() else "cpu"
-    num_workers = 4
-    num_epochs = 50
-    save_step = 1
-    out_directory = r".\out"
-    pretrained_model_path = None
-    # pretrained_model_path = r"C:\_Project\Pycharm Projects\MobileNet\pretrained\wjc-mobilenet-a100-r224-c1000-e0000.pth"
+    data = configs.data
+    model_type = configs.model
+    device = configs.device if torch.cuda.is_available() else "cpu"
+    num_workers = configs.num_workers
+    num_epochs = configs.num_epochs
+    save_step = configs.save_step
+    out_directory = configs.out_dir
+    pretrained_model_path = configs.resume
 
-    assert data in ["dogs", "mnist", "cifar10", "imagenet"]
-    assert model_type in ["mobile_net", "lenet"]
+    assert data in DATASETS.keys()
+    assert model_type in MODELS.keys()
 
     # training configs
     c = dict()
-    c["train_print_step"] = 100
+    c["print_step_train"] = configs.print_step
     c["device"] = device
 
-    out_path = Path(out_directory) / f"{find_next_id(Path(out_directory)):04d}"
-
     # select dataset
-    train_dataset = None
-    test_dataset = None
-    if data == "dogs":
-        train_dataset = DogsDataset(root=r"D:\_Dataset\Stanford Dogs", is_train=True)
-        test_dataset = DogsDataset(root=r"D:\_Dataset\Stanford Dogs", is_train=False)
-    elif data == "mnist":
-        train_dataset = MNISTDataset(root=r"D:\_Dataset", is_train=True)
-        test_dataset = MNISTDataset(root=r"D:\_Dataset", is_train=False)
-    elif data == "cifar10":
-        train_dataset = CIFAR10Dataset(root=r"D:\_Dataset\CIFAR10", is_train=True)
-        test_dataset = CIFAR10Dataset(root=r"D:\_Dataset\CIFAR10", is_train=False)
-    elif data == "imagenet":
-        train_dataset = ImageNetDataset(root=r"D:\_Dataset\ImageNet_2012", is_train=True)
-        test_dataset = ImageNetDataset(root=r"D:\_Dataset\ImageNet_2012", is_train=False)
+    Dataset = DATASETS[data]["class"]
+    train_root = DATASETS[data]["train_root"]
+    test_root = DATASETS[data]["test_root"]
+    train_dataset = Dataset(root=train_root, is_train=True)
+    test_dataset = Dataset(root=test_root, is_train=False)
     num_class = train_dataset.num_labels
 
     # set up model, dataloader, optimizer, criterion
-    network = None
-    if model_type == "lenet":
-        network = LeNet(num_class).to(device)
-    elif model_type == "mobile_net":
-        network = MobileNet(num_class).to(device)
+    Network = MODELS[model_type]
+    network = Network(num_class).to(device)
     train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)
     test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers)
     optimizer = optim.Adam(network.parameters(), lr=lr)
@@ -83,7 +90,8 @@
         print(f"{'-' * 5} Validation result {'-' * 5}")
         test_loop(network, test_dataloader, criterion, **c)
 
-        if epoch % save_step == 0:
+        if save_step is not None and out_directory is not None and epoch % save_step == 0:
+            out_path = Path(out_directory) / f"{find_next_id(Path(out_directory)):04d}"
             out_path.mkdir(exist_ok=True, parents=True)
             save_to = out_path / f"{model_type}-a{alpha * 100:3d}-r{input_resolution:d}-c{num_class}-e{epoch:04d}.pth"
             print(f"{'-' * 5} Saving model to {save_to} {'-' * 5}")
@@ -95,4 +103,5 @@
 
 
 if __name__ == '__main__':
-    train()
+    args = parser.parse_args()
+    train(args)
Index: test.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\"\"\"\r\noriginal preprocessing method is same as Inception, which takes 4×3×6×2 = 144 crops per image\r\nwe only take\r\n\"\"\"\r\n\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.utils.data import DataLoader\r\nimport torchvision.models as models\r\n\r\nfrom model import *\r\nfrom dataset import *\r\nfrom script import test_loop\r\nfrom script.utils import be_deterministic\r\n\r\nbe_deterministic()\r\n\r\n\r\ndef test():\r\n    # hyper parameters\r\n    batch_size = 64\r\n\r\n    # experiment settings\r\n    data = \"imagenet\"  # [\"dogs\", \"mnist\", \"cifar10\", \"imagenet\"]\r\n    model_type = \"mobile_net\"  # [\"mobile_net\", \"lenet\"]\r\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\n    num_workers = 4\r\n    pretrained_model_path = r\"C:\\_Project\\Pycharm Projects\\MobileNet\\pretrained\\wjc-mobilenet-a100-r224-c1000-e0000.pth\"\r\n\r\n    assert data in [\"dogs\", \"mnist\", \"cifar10\", \"imagenet\"]\r\n    assert model_type in [\"mobile_net\", \"lenet\"]\r\n\r\n    # training configs\r\n    c = dict()\r\n    c[\"test_print_step\"] = 20\r\n    c[\"device\"] = device\r\n\r\n    # select dataset\r\n    test_dataset = None\r\n    if data == \"dogs\":\r\n        test_dataset = DogsDataset(root=r\"D:\\_Dataset\\Stanford Dogs\", is_train=False)\r\n    elif data == \"mnist\":\r\n        test_dataset = MNISTDataset(root=r\"D:\\_Dataset\", is_train=False)\r\n    elif data == \"cifar10\":\r\n        test_dataset = CIFAR10Dataset(root=r\"D:\\_Dataset\\CIFAR10\", is_train=False)\r\n    elif data == \"imagenet\":\r\n        test_dataset = ImageNetDataset(root=r\"D:\\_Dataset\\ImageNet_2012\", is_train=False)\r\n    num_class = test_dataset.num_labels\r\n\r\n    # set up model, dataloader, optimizer, criterion\r\n    network = None\r\n    if model_type == \"lenet\":\r\n        network = LeNet(num_class).to(device)\r\n    elif model_type == \"mobile_net\":\r\n        network = MobileNet(num_class).to(device)\r\n    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers)\r\n    criterion = nn.CrossEntropyLoss()\r\n\r\n    state = torch.load(pretrained_model_path)\r\n    network.load_state_dict(state[\"state_dict\"])\r\n\r\n    network = models.mobilenet_v2(pretrained=True)\r\n    network = network.to(device)\r\n\r\n    # testing loop\r\n    print(f\"{'-' * 5} Test result {'-' * 5}\")\r\n    test_loop(network, test_dataloader, criterion, **c)\r\n\r\n\r\nif __name__ == '__main__':\r\n    test()\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/test.py b/test.py
--- a/test.py	(revision 11f2caf4762201f1e4e6ca9dd1ab2f33af88b09f)
+++ b/test.py	(date 1655805892635)
@@ -1,6 +1,6 @@
 """
 original preprocessing method is same as Inception, which takes 4×3×6×2 = 144 crops per image
-we only take
+we only take 1 center crop after resizing
 """
 
 import torch
@@ -21,7 +21,7 @@
     batch_size = 64
 
     # experiment settings
-    data = "imagenet"  # ["dogs", "mnist", "cifar10", "imagenet"]
+    data = "imagenet"  # ["stanford-dogs", "mnist", "cifar10", "imagenet"]
     model_type = "mobile_net"  # ["mobile_net", "lenet"]
     device = "cuda" if torch.cuda.is_available() else "cpu"
     num_workers = 4
@@ -32,12 +32,12 @@
 
     # training configs
     c = dict()
-    c["test_print_step"] = 20
+    c["print_step_test"] = 20
     c["device"] = device
 
     # select dataset
     test_dataset = None
-    if data == "dogs":
+    if data == "stanford-dogs":
         test_dataset = DogsDataset(root=r"D:\_Dataset\Stanford Dogs", is_train=False)
     elif data == "mnist":
         test_dataset = MNISTDataset(root=r"D:\_Dataset", is_train=False)
Index: train.sh
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/train.sh b/train.sh
new file mode 100644
--- /dev/null	(date 1655805799112)
+++ b/train.sh	(date 1655805799112)
@@ -0,0 +1,5 @@
+python train.py \
+--data stanford-dog \
+--model mobile-net \
+--batch-size 64 \
+--print-step 50
\ No newline at end of file
